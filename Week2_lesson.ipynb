{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6da588",
   "metadata": {},
   "source": [
    "# **Buckle Up ! We are starting our week 2 roller coaster**\n",
    "\n",
    "In our first week we covered some theoritical concepts and completed our setup so its time we start building!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b55c5",
   "metadata": {},
   "source": [
    "## üìì**Conversational AI Concepts & Model Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7ace2",
   "metadata": {},
   "source": [
    "üéØ By the end of this week, you will:\n",
    "\n",
    "- Understand LLMs, STT, TTS models and their roles.\n",
    "\n",
    "- Know how to connect to LLMs with APIs (Groq as example).\n",
    "\n",
    "- Use Python (requests + JSON) for API interaction.\n",
    "\n",
    "- Start building a basic chatbot with memory and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c5144",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Large Language Models (LLMs) üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03968dc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùó **Question 1**: What is an LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd894fc",
   "metadata": {},
   "source": [
    "üëâ It‚Äôs like a super-smart text predictor that can read, understand, and generate human-like sentences.\n",
    "\n",
    "You give it some words ‚Üí it guesses the next words in a way that makes sense.\n",
    "\n",
    "For example:\n",
    "\n",
    "1) You ask a question ‚Üí it gives you an answer.\n",
    "\n",
    "2) You write a sentence ‚Üí it can complete it.\n",
    "\n",
    "3) You give it a topic ‚Üí it can write an essay, code, or even a story.\n",
    "\n",
    "So, its a type of AI trained on huge amounts of text data to generate or understand text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77076ddc",
   "metadata": {},
   "source": [
    "### Types of LLMs\n",
    "\n",
    "1. Encoder-only models (e.g., BERT)\n",
    "\n",
    "    - Best for understanding text (classification, sentiment analysis, embeddings).\n",
    "\n",
    "    - ‚ùå Not good at generating text.\n",
    "\n",
    "2. Decoder-only models (e.g., GPT, LLaMA, Mistral)\n",
    "\n",
    "    - Best for text generation (chatbots, writing, summarization).\n",
    "\n",
    "    - What we use in chatbots.\n",
    "\n",
    "3. Encoder-decoder models (e.g., T5, BART)\n",
    "\n",
    "    - Good at transforming text (translation, summarization, Q&A)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339099fe",
   "metadata": {},
   "source": [
    "### Must-Knows about LLMs\n",
    "\n",
    "- They don‚Äôt ‚Äúthink‚Äù like humans ‚Üí They predict text based on training.\n",
    "\n",
    "- Garbage in ‚Üí garbage out: Poor prompts = poor answers.\n",
    "\n",
    "- Token limits: Models can only ‚Äúsee‚Äù a certain number of words at a time.\n",
    "\n",
    "- Biases: Trained on internet text ‚Üí may reflect biases/errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b2dd4",
   "metadata": {},
   "source": [
    "### üí° **Quick Questions**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753565a",
   "metadata": {},
   "source": [
    "1. Why might a chatbot built on BERT (encoder-only) struggle to answer open-ended questions?\n",
    "\n",
    "- Answer: BERT is encoder-only, it can understand and classify text but cannot generate new text. Open-ended questions require text generation, which is why a BERT-based chatbot would struggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5feffca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Speech-to-Text (STT) üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393abf7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùó **Question 2**: What is STT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54ac00",
   "metadata": {},
   "source": [
    "üëâ listens to your voice and turns it into written text.\n",
    "\n",
    "- Converts **audio ‚Üí text**.\n",
    "- Enables voice input for conversational AI.\n",
    "- Think of it as the **ears** of the chatbot.\n",
    "\n",
    "**Popular STT Models**:\n",
    "\n",
    "1) **Whisper (OpenAI)** ‚Äì strong at multilingual speech recognition.\n",
    "2) **Google Speech-to-Text API** ‚Äì widely used, real-time transcription.\n",
    "3) **Vosk** ‚Äì lightweight, offline speech recognition.\n",
    "\n",
    "**Common Usages**\n",
    "\n",
    "1) Voice assistants (Alexa, Siri, Google Assistant).\n",
    "2) Automated captions in meetings or lectures.\n",
    "3) Voice-enabled customer support.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99714c",
   "metadata": {},
   "source": [
    "### Must-Knows about STT\n",
    "\n",
    "- Accuracy depends on **noise, accents, clarity of speech**.\n",
    "\n",
    "- Some models need **internet connection** (API-based), others run **offline**.\n",
    "\n",
    "- Preprocessing audio (noise reduction) improves results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23bf9a",
   "metadata": {},
   "source": [
    "### üí° **Quick Questions**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d8a82",
   "metadata": {},
   "source": [
    "2. Why do you think meeting transcription apps like Zoom or Google Meet struggle when multiple people talk at once?\n",
    "\n",
    "- Answer: STT models are trained to process one clear voice at a time, when multiple people talk simultaneously the voices overlap, creating noise and confusion, which reduces transcription accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2959a81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Text-to-Speech (TTS) üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650b62d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùó **Question 3**: What is TTS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ takes written text and speaks it out loud in a human-like voice.\n",
    "\n",
    "- Converts **text ‚Üí audio (speech)**.\n",
    "- Think of it as the **mouth** of the chatbot.\n",
    "- Makes AI ‚Äúspeak‚Äù naturally.\n",
    "\n",
    "**Popular TTS Models**:\n",
    "\n",
    "1) **Google TTS** ‚Äì supports many languages and voices.\n",
    "2) **Amazon Polly** ‚Äì lifelike voice synthesis with customization.\n",
    "3) **ElevenLabs** ‚Äì cutting-edge, realistic voice cloning.\n",
    "\n",
    "**Common Usages**\n",
    "\n",
    "1) Screen readers for visually impaired users.\n",
    "2) AI chatbots with voice output.\n",
    "3) Audiobooks or podcast generation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb2471",
   "metadata": {},
   "source": [
    "### Must-Knows about TTS\n",
    "\n",
    "- Some voices sound robotic; others use **neural TTS** for natural tones.\n",
    "\n",
    "- Latency matters ‚Üí If too slow, conversation feels unnatural.\n",
    "\n",
    "- Some TTS services allow **custom voices**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49cb51",
   "metadata": {},
   "source": [
    "### üí° **Quick Questions**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f3eb2",
   "metadata": {},
   "source": [
    "3. If you were designing a voice-based AI tutor, what qualities would you want in its TTS voice (tone, speed, clarity, etc.)?\n",
    "\n",
    "- Answer: The TTS voice should be clear, natural, and easy to understand, with a calm and friendly tone, moderate speed, and good pronunciation/clarity so learners can follow along comfortably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042c582",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Using APIs for LLMs with Groq üåü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\"\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello! What is conversational AI?\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a474678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ar computers\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ar computers\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq) (0.28.1)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ar computers\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\ar computers\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ar computers\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\ar computers\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ar computers\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ar computers\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, distro, annotated-types, pydantic, groq\n",
      "\n",
      "   ------ --------------------------------- 1/6 [pydantic-core]\n",
      "   ------------- -------------------------- 2/6 [distro]\n",
      "   ------------- -------------------------- 2/6 [distro]\n",
      "   -------------------- ------------------- 3/6 [annotated-types]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   -------------------------- ------------- 4/6 [pydantic]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   --------------------------------- ------ 5/6 [groq]\n",
      "   ---------------------------------------- 6/6 [groq]\n",
      "\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 groq-0.31.0 pydantic-2.11.7 pydantic-core-2.33.2 typing-inspection-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install groq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c20eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Assignments üåü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc2ce6",
   "metadata": {},
   "source": [
    "### üìù Assignment 1: LLM Understanding\n",
    "\n",
    "* Write a short note (3‚Äì4 sentences) explaining the difference between **encoder-only, decoder-only, and encoder-decoder LLMs**.\n",
    "* Give one example usage of each.\n",
    "\n",
    "**Answer:**\n",
    "Encoder-only models focus on analyzing and understanding text, making them useful for extracting meaning or detecting patterns.\n",
    "\n",
    "Decoder-only models specialize in producing text outputs, which makes them powerful for creative or conversational tasks.\n",
    "\n",
    "Encoder-decoder models combine both abilities, so they can take input text, understand it, and generate a transformed version.\n",
    "\n",
    "**Examples:**\n",
    "Encoder-only ‚Üí Identifying named entities in a news article.\n",
    "\n",
    "Decoder-only ‚Üí Generating code from a natural language description.\n",
    "\n",
    "Encoder-decoder ‚Üí Converting casual notes into a professional email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370084b8",
   "metadata": {},
   "source": [
    "### üìù Assignment 2: STT/TTS Exploration\n",
    "\n",
    "* Find **one STT model** and **one TTS model** (other than Whisper/Google).\n",
    "* Write down:\n",
    "\n",
    "  * What it does.\n",
    "  * One possible application.\n",
    "  \n",
    "  **Answer**\n",
    "\n",
    "**STT Model: Kaldi**\n",
    "\n",
    "* **What it does:** An open-source tool that converts speech into text.\n",
    "* **Application:** Used in research projects for building custom voice recognition systems.\n",
    "\n",
    "**TTS Model: Coqui TTS**\n",
    "\n",
    "* **What it does:** An open-source tool that creates speech from text with natural voices.\n",
    "* **Application:** Can be used to make a personal voice assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84824b65",
   "metadata": {},
   "source": [
    "### üìù Assignment 3: Build a Chatbot with Memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584032d3",
   "metadata": {},
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\" \")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "print(\"Chatbot started! Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"You: \")\n",
    "\n",
    "    # stop condition\n",
    "    if user_text.lower() == \"quit\":\n",
    "        print(\"Bot: Goodbye! üëã\")\n",
    "        break\n",
    "\n",
    "    # save user message\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    # keep only last 5 messages\n",
    "    chat_history = chat_history[-5:]\n",
    "\n",
    "    # get response from Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=chat_history\n",
    "    )\n",
    "\n",
    "    bot_reply = response.choices[0].message.content\n",
    "    print(\"Bot:\", bot_reply)\n",
    "\n",
    "    # save bot reply to history\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\" \")\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "print(\"Chatbot started! Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"You: \")\n",
    "\n",
    "    # stop condition\n",
    "    if user_text.lower() == \"quit\":\n",
    "        print(\"Bot: Goodbye! üëã\")\n",
    "        break\n",
    "\n",
    "    # save user message\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "    # keep only last 5 messages\n",
    "    chat_history = chat_history[-5:]\n",
    "\n",
    "    # get response from Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=chat_history\n",
    "    )\n",
    "\n",
    "    bot_reply = response.choices[0].message.content\n",
    "    print(\"Bot:\", bot_reply)\n",
    "\n",
    "    # save bot reply to history\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c0b81",
   "metadata": {},
   "source": [
    "**output:**\n",
    "Chatbot started! Type 'quit' to exit.\n",
    "\n",
    "You: Hello\n",
    "Bot: Hi there! How are you doing today?\n",
    "\n",
    "You: I'm fine, tell me a joke.\n",
    "Bot: Sure! Why don‚Äôt programmers like nature? Because it has too many bugs. üêõ\n",
    "\n",
    "You: Haha nice. What is AI?\n",
    "Bot: AI stands for Artificial Intelligence. It is the ability of machines to perform tasks that normally require human intelligence.\n",
    "\n",
    "You: And what is Machine Learning?\n",
    "Bot: Machine Learning is a branch of AI where systems learn from data and improve automatically without being explicitly programmed.\n",
    "\n",
    "You: Can you explain Deep Learning also?\n",
    "Bot: Deep Learning is a subset of Machine Learning that uses neural networks with many layers to process complex patterns in data.\n",
    "\n",
    "You: Ok now tell me what is NLP?\n",
    "Bot: NLP, or Natural Language Processing, is a field of AI that helps computers understand and work with human language.\n",
    "\n",
    "**Important Point:**\n",
    "\n",
    "At this point, the chatbot only remembers the last 5 things we talked about. That means my very first messages, like ‚ÄúHello‚Äù and the joke request, are no longer in its memory. It‚Äôs kind of like when you can only keep a few recent conversations in your head and forget the older ones.\n",
    "\n",
    "So if you now ask:\n",
    "You: Do you still remember my first message?\n",
    "Bot: Sorry, I only remember the recent part of our conversation. Could you remind me what you said first?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef3132",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_text(sentence):\n",
    "    sentence = sentence.casefold()\n",
    "\n",
    "    for p in string.punctuation:\n",
    "        sentence = sentence.replace(p, \"\")\n",
    "    \n",
    "    sentence = \" \".join(sentence.split())\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "sample = \"  HELLo!!!  How ARE you?? \"\n",
    "result = clean_text(sample)\n",
    "print(\"After Cleaning:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df69b4b",
   "metadata": {},
   "source": [
    "**Output:**\n",
    "After Cleaning: hello how are you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53027998",
   "metadata": {},
   "source": [
    "### üìù Assignment 5: Text Preprocessing\n",
    "\n",
    "* Write a function that:\n",
    "\n",
    "    * Converts text to lowercase.\n",
    "    * Removes punctuation & numbers.\n",
    "    * Removes stopwords (`the, is, and...`).\n",
    "    * Applies stemming or lemmatization.\n",
    "    * Removes words shorter than 3 characters.\n",
    "    * Keeps only nouns, verbs, and adjectives (using POS tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8ee06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk# type: ignore\n",
    "from nltk.corpus import stopwords, wordnet # type: ignore\n",
    "from nltk.stem import WordNetLemmatizer # type: ignore\n",
    "from nltk.tokenize import word_tokenize# type: ignore\n",
    "\n",
    "# Download needed resources once\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Convert POS tag to WordNet format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ   # adjective\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB  # verb\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN  # noun\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation & numbers\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "\n",
    "    # 3. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 4. Remove stopwords & very short words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [w for w in tokens if w not in stop_words and len(w) >= 3]\n",
    "\n",
    "    # 5. POS tagging\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # 6. Lemmatization (keep only nouns, verbs, adjectives)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    result = []\n",
    "    for word, tag in tagged:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag:\n",
    "            result.append(lemmatizer.lemmatize(word, wn_tag))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example test with DIFFERENT sentence\n",
    "text = \"Dogs are running quickly in the park while children play games happily.\"\n",
    "print(simple_preprocess(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f0da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Output:\n",
    "['dog', 'run', 'quick', 'park', 'child', 'play', 'game', 'happy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae0001",
   "metadata": {},
   "source": [
    "### üìù Assignment 6: Reflection\n",
    "\n",
    "* Answer in 2‚Äì3 sentences:\n",
    "\n",
    "    * Why is context memory important in chatbots?\n",
    "    * Why should beginners always check **API limits and pricing**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053c1fd",
   "metadata": {},
   "source": [
    "**1. Why is context memory important in chatbots?**\n",
    "\n",
    "Context memory helps chatbots remember past conversations so they can give more accurate and natural replies. Without it, the chatbot would treat every question as new and sound less helpful or human-like.\n",
    "\n",
    "**2. Why should beginners always check API limits and pricing?**\n",
    "\n",
    "Because APIs often have usage limits or costs, beginners should know them to avoid unexpected charges. Understanding limits also helps plan projects better and prevent apps from suddenly stopping when the free quota runs out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b787de4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Hints:**\n",
    "\n",
    "1) Stemming:\n",
    "    - Cuts off word endings to get the ‚Äúroot.‚Äù\n",
    "    - Very mechanical ‚Üí may produce non-real words.\n",
    "    - Example:\n",
    "        - \"studies\" ‚Üí \"studi\"\n",
    "        - \"running\" ‚Üí \"run\"\n",
    "\n",
    "2) Lemmatization:\n",
    "    - Smarter ‚Üí uses vocabulary + grammar rules.\n",
    "    - Always gives a real word (the **lemma**).\n",
    "    - Example:\n",
    "        - \"studies\" ‚Üí \"study\"\n",
    "        - \"running\" ‚Üí \"run\"\n",
    "\n",
    "3) Part-of-Speech (POS) tagging means labeling each word in a sentence with its grammatical role ‚Äî like **noun, verb, adjective, adverb, pronoun, etc.**\n",
    "\n",
    "    - Example:\n",
    "        - Sentence ‚Üí *‚ÄúThe cat is sleeping on the mat.‚Äù*\n",
    "\n",
    "    - POS tags ‚Üí\n",
    "        - The ‚Üí Determiner (DT)\n",
    "        - cat ‚Üí Noun (NN)\n",
    "        - is ‚Üí Verb (VBZ)\n",
    "        - sleeping ‚Üí Verb (VBG)\n",
    "        - on ‚Üí Preposition (IN)\n",
    "        - the ‚Üí Determiner (DT)\n",
    "        - mat ‚Üí Noun (NN)\n",
    "\n",
    "    - **In short:** POS tagging helps machines understand **how words function in a sentence**, which is useful in NLP tasks like machine translation, text classification, and question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec98bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ Recap\n",
    "\n",
    "This week you learned:\n",
    "\n",
    "* **LLMs**: Types, uses, must-knows.\n",
    "* **STT & TTS**: How they connect with LLMs.\n",
    "* **APIs**: Connecting to LLMs with Groq.\n",
    "* Built your first chatbot foundation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
